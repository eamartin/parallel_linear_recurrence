\relax 
\providecommand\hyper@newdestlabel[2]{}
\bibstyle{abbrvnat}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{sutskever2014sequence}
\citation{amodei2015deep}
\citation{hausknecht2015deep}
\citation{hochreiter1997long}
\citation{cho2014learning}
\citation{keskar2017large}
\citation{diamos2016persistent}
\citation{balduzzi2016strongly}
\citation{bradbury2017quasi}
\citation{kalchbrenner2016neural}
\citation{gehring2017convolutional}
\citation{van2016wavenet}
\@writefile{tdo}{\contentsline {todo}{Thought: we could rephrase this as allowing us to break the necessity of using large batches to train fast}{1}{section*.1}}
\pgfsyspdfmark {pgfid1}{13351168}{31411448}
\pgfsyspdfmark {pgfid4}{35988437}{31403580}
\pgfsyspdfmark {pgfid5}{38101972}{31190265}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{ladner1980parallel}
\citation{blelloch1990prefix}
\citation{bradbury2017quasi}
\citation{lei2017}
\citation{blelloch1990prefix}
\@writefile{toc}{\contentsline {section}{\numberline {2}Parallel linear recurrence}{2}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Theoretical performance}{2}{subsection.2.1}}
\citation{abadi2016tensorflow}
\newlabel{alg:plr}{{2}{3}{Parallel linear recurrence}{section.2}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Parallel linear recurrence on $p$ processors}}{3}{algorithm.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Backpropagation}{3}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Implementation}{3}{subsection.2.3}}
\citation{bradbury2017quasi}
\citation{balduzzi2016strongly}
\@writefile{toc}{\contentsline {section}{\numberline {3}Models}{4}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Gated impulse linear recurrent layer}{4}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Impact on effective "batch size"}{4}{subsubsection.3.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Linear surrogate RNNs}{4}{subsection.3.2}}
\newlabel{sec:ls-rnns}{{3.2}{4}{Linear surrogate RNNs}{subsection.3.2}{}}
\citation{hochreiter1997long}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{5}{section.4}}
\@writefile{tdo}{\contentsline {todo}{I hope so\ldots  }{5}{section*.2}}
\pgfsyspdfmark {pgfid6}{19219914}{15540101}
\pgfsyspdfmark {pgfid9}{35988437}{15532233}
\pgfsyspdfmark {pgfid10}{38101972}{15318918}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Throughput benchmarks}{5}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Kernel performance}{5}{subsubsection.4.1.1}}
\citation{lei2017}
\citation{bradbury2017quasi}
\citation{lei2017}
\citation{bradbury2017quasi}
\citation{hochreiter1997long}
\newlabel{table:kernel-throughput}{{4.1.1}{6}{Kernel performance}{subsubsection.4.1.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces  Speedup of the parallel kernel compared to the serial kernel at various values of $(\text  {minibatch size } \mathbf  {x} \text  { features}) = m$. Performance was directly measured without any overhead from TensorFlow. }}{6}{table.1}}
\newlabel{table:rnn-throughput}{{4.1.2}{6}{Accelerating existing RNN architectures}{subsubsection.4.1.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces  Speedup obtained from using parallel linear recurrence rather than serial linear recurrence. We analyse this for SRUs \cite  {lei2017} and QRNNs \cite  {bradbury2017quasi} with a convolutional filters of size 2 and 10, and GILR-LSTM which we introduce in section \ref  {sec:ls-rnns}. We controlled for GPU memory by setting minibatch size $b$ such that $bT = 65536$ for sequence length $T$. }}{6}{table.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Accelerating existing RNN architectures}{6}{subsubsection.4.1.2}}
\@writefile{tdo}{\contentsline {todo}{For table \ref  {table:rnn-throughput} how many layers and what was hidden size?}{6}{section*.3}}
\pgfsyspdfmark {pgfid11}{7104430}{20979418}
\pgfsyspdfmark {pgfid14}{35988437}{20971550}
\pgfsyspdfmark {pgfid15}{38101972}{20758235}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}GILR-LSTM throughput}{6}{subsubsection.4.1.3}}
\@writefile{tdo}{\contentsline {todo}{Put title on Figure \ref  {fig:tp_perf}}{6}{section*.4}}
\pgfsyspdfmark {pgfid16}{7104430}{10902423}
\pgfsyspdfmark {pgfid19}{35988437}{10894555}
\pgfsyspdfmark {pgfid20}{38101972}{10681240}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Synthetic Experiment}{6}{subsection.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Throughput comparison between LSTM-256-256 with GILR-LSTM-256-256, with a 32-dimensional input and a 2-dimensional output, for various batch sizes and sequence lengths.The LSTM only has a single row of data because its throughput is independent of sequence length. Entries are missing from the GILR-LSTM table because there was not enough memory on the GPU to handle such large batch sizes and sequences.}}{7}{figure.1}}
\newlabel{fig:tp_perf}{{1}{7}{Throughput comparison between LSTM-256-256 with GILR-LSTM-256-256, with a 32-dimensional input and a 2-dimensional output, for various batch sizes and sequence lengths.The LSTM only has a single row of data because its throughput is independent of sequence length. Entries are missing from the GILR-LSTM table because there was not enough memory on the GPU to handle such large batch sizes and sequences}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Training curves for GILR-LSTM and CUDNNLSTM architectures for various sequence lengths. (Clockwise from top-left: 1024, 8192, 1048576)}}{8}{figure.2}}
\newlabel{fig:synthetic_training}{{2}{8}{Training curves for GILR-LSTM and CUDNNLSTM architectures for various sequence lengths. (Clockwise from top-left: 1024, 8192, 1048576)}{figure.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Performance of the GILR-LSTM compared to CUDA-optimised CudnnLSTM implementation on problem 2b from (cite '97). We see that the GILR-LSTM has a clear advantage in training speed as the sequence length increases. We did not observe the Cudnn-LSTM converging for the longest task, even after several days. {\relax \fontsize  {7}{8}\selectfont  *For the longest sequence length, the number of hidden units was decreased to 64 for both architectures so that the net could fit in memory. } }}{8}{table.3}}
\newlabel{my-label}{{3}{8}{Performance of the GILR-LSTM compared to CUDA-optimised CudnnLSTM implementation on problem 2b from (cite '97). We see that the GILR-LSTM has a clear advantage in training speed as the sequence length increases. We did not observe the Cudnn-LSTM converging for the longest task, even after several days. {\scriptsize *For the longest sequence length, the number of hidden units was decreased to 64 for both architectures so that the net could fit in memory. }}{table.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}The Medical Setting}{8}{subsection.4.3}}
\citation{lei2017}
\citation{lei2017}
\bibcite{abadi2016tensorflow}{{1}{2016}{{Abadi et~al.}}{{Abadi, Agarwal, Barham, Brevdo, Chen, Citro, Corrado, Davis, Dean, Devin, et~al.}}}
\bibcite{amodei2015deep}{{2}{2015}{{Amodei et~al.}}{{Amodei, Anubhai, Battenberg, Case, Casper, Catanzaro, Chen, Chrzanowski, Coates, Diamos, et~al.}}}
\bibcite{balduzzi2016strongly}{{3}{2016}{{Balduzzi and Ghifary}}{{}}}
\bibcite{blelloch1990prefix}{{4}{1990}{{Blelloch}}{{}}}
\bibcite{bradbury2017quasi}{{5}{2017}{{Bradbury et~al.}}{{Bradbury, Merity, Xiong, and Socher}}}
\bibcite{cho2014learning}{{6}{2014}{{Cho et~al.}}{{Cho, Van~Merri{\"e}nboer, Gulcehre, Bahdanau, Bougares, Schwenk, and Bengio}}}
\bibcite{diamos2016persistent}{{7}{2016}{{Diamos et~al.}}{{Diamos, Sengupta, Catanzaro, Chrzanowski, Coates, Elsen, Engel, Hannun, and Satheesh}}}
\@writefile{tdo}{\contentsline {todo}{If we get good results (doesn't have to be best ever, but can be on par), then keep}{9}{section*.5}}
\pgfsyspdfmark {pgfid21}{7104430}{38313985}
\@writefile{tdo}{\contentsline {todo}{If we don't get good results, or don't have results, we will need to kick this section}{9}{section*.6}}
\pgfsyspdfmark {pgfid26}{7104430}{38313985}
\pgfsyspdfmark {pgfid24}{35988437}{38306117}
\pgfsyspdfmark {pgfid25}{38101972}{38092802}
\pgfsyspdfmark {pgfid29}{35988437}{32581889}
\pgfsyspdfmark {pgfid30}{38101972}{32368574}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{9}{section.5}}
\bibcite{gehring2017convolutional}{{8}{2017}{{Gehring et~al.}}{{Gehring, Auli, Grangier, Yarats, and Dauphin}}}
\bibcite{glorot2010understanding}{{9}{2010}{{Glorot and Bengio}}{{}}}
\bibcite{hausknecht2015deep}{{10}{2015}{{Hausknecht and Stone}}{{}}}
\bibcite{hochreiter1997long}{{11}{1997}{{Hochreiter and Schmidhuber}}{{}}}
\bibcite{kalchbrenner2016neural}{{12}{2016}{{Kalchbrenner et~al.}}{{Kalchbrenner, Espeholt, Simonyan, Oord, Graves, and Kavukcuoglu}}}
\bibcite{keskar2017large}{{13}{2017}{{Keskar et~al.}}{{Keskar, Mudigere, Nocedal, Smelyanskiy, and Tang}}}
\bibcite{kingma2014adam}{{14}{2014}{{Kingma and Ba}}{{}}}
\bibcite{ladner1980parallel}{{15}{1980}{{Ladner and Fischer}}{{}}}
\bibcite{lecun1998mnist}{{16}{1998}{{LeCun et~al.}}{{LeCun, Cortes, and Burges}}}
\bibcite{orchard2015converting}{{17}{2015}{{Orchard et~al.}}{{Orchard, Jayawant, Cohen, and Thakor}}}
\bibcite{sutskever2014sequence}{{18}{2014}{{Sutskever et~al.}}{{Sutskever, Vinyals, and Le}}}
\bibcite{van2016wavenet}{{19}{2016}{{van~den Oord et~al.}}{{van~den Oord, Dieleman, Zen, Simonyan, Vinyals, Graves, Kalchbrenner, Senior, and Kavukcuoglu}}}
\bibcite{lei2017}{{20}{2017}{{Lei and Zhang }}{{}}}
\bibcite{physiobank}{{21}{2000}{{Goldberger et~al.}}{{ Amaral, Glass, Hausdorff, Ivanov, Mark, Mietus, Moody, Peng, Stanley}}}
