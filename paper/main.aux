\relax 
\providecommand\hyper@newdestlabel[2]{}
\bibstyle{abbrvnat}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{sutskever2014sequence}
\citation{amodei2015deep}
\citation{hausknecht2015deep}
\citation{hochreiter1997long}
\citation{cho2014learning}
\citation{keskar2017large}
\citation{diamos2016persistent}
\citation{balduzzi2016strongly}
\citation{bradbury2017quasi}
\citation{kalchbrenner2016neural}
\citation{gehring2017convolutional}
\citation{van2016wavenet}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{ladner1980parallel}
\citation{blelloch1990prefix}
\citation{bradbury2017quasi}
\citation{lei2017}
\citation{blelloch1990prefix}
\@writefile{toc}{\contentsline {section}{\numberline {2}Parallel linear recurrence}{2}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Theoretical performance}{2}{subsection.2.1}}
\citation{abadi2016tensorflow}
\newlabel{alg:plr}{{2}{3}{Parallel linear recurrence}{section.2}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Parallel linear recurrence on $p$ processors}}{3}{algorithm.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Backpropagation}{3}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Implementation}{3}{subsection.2.3}}
\citation{bradbury2017quasi}
\citation{balduzzi2016strongly}
\@writefile{toc}{\contentsline {section}{\numberline {3}Models}{4}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Gated impulse linear recurrent layer}{4}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Impact on effective "batch size"}{4}{subsubsection.3.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Linear surrogate RNNs}{4}{subsection.3.2}}
\newlabel{sec:ls-rnns}{{3.2}{4}{Linear surrogate RNNs}{subsection.3.2}{}}
\citation{hochreiter1997long}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{5}{section.4}}
\@writefile{tdo}{\contentsline {todo}{I hope so\ldots  }{5}{section*.1}}
\pgfsyspdfmark {pgfid1}{17400003}{14316694}
\pgfsyspdfmark {pgfid4}{34093331}{14308826}
\pgfsyspdfmark {pgfid5}{34311761}{14095511}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Throughput benchmarks}{5}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Kernel performance}{5}{subsubsection.4.1.1}}
\citation{hochreiter1997long}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Parallel kernel speedup on $m$ features (minibatch size $= 1$)}}{6}{table.1}}
\newlabel{table:kernel-throughput}{{1}{6}{Parallel kernel speedup on $m$ features (minibatch size $= 1$)}{table.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Parallel kernel speedup for a variety of LS-RNNs}}{6}{table.2}}
\newlabel{table:rnn-throughput}{{2}{6}{Parallel kernel speedup for a variety of LS-RNNs}{table.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Accelerating existing RNN architectures}{6}{subsubsection.4.1.2}}
\@writefile{tdo}{\contentsline {todo}{For table \ref  {table:rnn-throughput} how many layers and what was hidden size?}{6}{section*.2}}
\pgfsyspdfmark {pgfid6}{7104429}{15725006}
\pgfsyspdfmark {pgfid9}{34093331}{15717138}
\pgfsyspdfmark {pgfid10}{34311761}{15503823}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}GILR-LSTM throughput}{6}{subsubsection.4.1.3}}
\@writefile{tdo}{\contentsline {todo}{I think we should delete this section. Already over length limit, doesn't show anything great}{6}{section*.3}}
\pgfsyspdfmark {pgfid11}{7104429}{12697354}
\pgfsyspdfmark {pgfid14}{34093331}{4870399}
\pgfsyspdfmark {pgfid15}{34311761}{4657084}
\@writefile{tdo}{\contentsline {todo}{Chris: Put title on Figure \ref  {fig:tp_perf}}{6}{section*.4}}
\pgfsyspdfmark {pgfid16}{7104429}{4308746}
\pgfsyspdfmark {pgfid19}{34093331}{-8785649}
\pgfsyspdfmark {pgfid20}{34311761}{-8998964}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Throughput comparison between LSTM-256-256 with GILR-LSTM-256-256, with a 32-dimensional input and a 2-dimensional output, for various batch sizes and sequence lengths.The LSTM only has a single row of data because its throughput is independent of sequence length. Entries are missing from the GILR-LSTM table because there was not enough memory on the GPU to handle such large batch sizes and sequences.}}{7}{figure.1}}
\newlabel{fig:tp_perf}{{1}{7}{Throughput comparison between LSTM-256-256 with GILR-LSTM-256-256, with a 32-dimensional input and a 2-dimensional output, for various batch sizes and sequence lengths.The LSTM only has a single row of data because its throughput is independent of sequence length. Entries are missing from the GILR-LSTM table because there was not enough memory on the GPU to handle such large batch sizes and sequences}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Synthetic Experiment}{7}{subsection.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Training curves for GILR-LSTM and CUDNNLSTM architectures for various sequence lengths. (Clockwise from top-left: 1024, 8192, 1048576)}}{8}{figure.2}}
\newlabel{fig:synthetic_training}{{2}{8}{Training curves for GILR-LSTM and CUDNNLSTM architectures for various sequence lengths. (Clockwise from top-left: 1024, 8192, 1048576)}{figure.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Performance of the GILR-LSTM compared to CUDA-optimised CudnnLSTM implementation on problem 2b from (cite '97). We see that the GILR-LSTM has a clear advantage in training speed as the sequence length increases. We did not observe the Cudnn-LSTM converging for the longest task, even after several days. {\relax \fontsize  {7}{8pt}\selectfont  *For the longest sequence length, the number of hidden units was decreased to 64 for both architectures so that the net could fit in memory. } }}{8}{table.3}}
\newlabel{my-label}{{3}{8}{Performance of the GILR-LSTM compared to CUDA-optimised CudnnLSTM implementation on problem 2b from (cite '97). We see that the GILR-LSTM has a clear advantage in training speed as the sequence length increases. We did not observe the Cudnn-LSTM converging for the longest task, even after several days. {\scriptsize *For the longest sequence length, the number of hidden units was decreased to 64 for both architectures so that the net could fit in memory. }}{table.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Abnormal heart sound detection}{8}{subsection.4.3}}
\citation{lei2017}
\bibcite{abadi2016tensorflow}{{1}{2016}{{Abadi et~al.}}{{Abadi, Agarwal, Barham, Brevdo, Chen, Citro, Corrado, Davis, Dean, Devin, et~al.}}}
\bibcite{amodei2015deep}{{2}{2015}{{Amodei et~al.}}{{Amodei, Anubhai, Battenberg, Case, Casper, Catanzaro, Chen, Chrzanowski, Coates, Diamos, et~al.}}}
\bibcite{balduzzi2016strongly}{{3}{2016}{{Balduzzi and Ghifary}}{{}}}
\bibcite{blelloch1990prefix}{{4}{1990}{{Blelloch}}{{}}}
\bibcite{bradbury2017quasi}{{5}{2017}{{Bradbury et~al.}}{{Bradbury, Merity, Xiong, and Socher}}}
\bibcite{cho2014learning}{{6}{2014}{{Cho et~al.}}{{Cho, Van~Merri{\"e}nboer, Gulcehre, Bahdanau, Bougares, Schwenk, and Bengio}}}
\bibcite{diamos2016persistent}{{7}{2016}{{Diamos et~al.}}{{Diamos, Sengupta, Catanzaro, Chrzanowski, Coates, Elsen, Engel, Hannun, and Satheesh}}}
\@writefile{tdo}{\contentsline {todo}{how many?}{9}{section*.5}}
\pgfsyspdfmark {pgfid21}{22755932}{32910622}
\pgfsyspdfmark {pgfid24}{34093331}{32902754}
\pgfsyspdfmark {pgfid25}{34311761}{32689439}
\@writefile{tdo}{\contentsline {todo}{If we get good results (doesn't have to be best ever, but can be on par), then keep}{9}{section*.6}}
\pgfsyspdfmark {pgfid26}{7104429}{31075614}
\@writefile{tdo}{\contentsline {todo}{If we don't get good results, or don't have results, we will need to kick this section}{9}{section*.7}}
\pgfsyspdfmark {pgfid31}{7104429}{31075614}
\pgfsyspdfmark {pgfid29}{34093331}{30763344}
\pgfsyspdfmark {pgfid30}{34311761}{30550029}
\pgfsyspdfmark {pgfid34}{34093331}{16368702}
\pgfsyspdfmark {pgfid35}{34311761}{16155387}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{9}{section.5}}
\bibcite{gehring2017convolutional}{{8}{2017}{{Gehring et~al.}}{{Gehring, Auli, Grangier, Yarats, and Dauphin}}}
\bibcite{glorot2010understanding}{{9}{2010}{{Glorot and Bengio}}{{}}}
\bibcite{hausknecht2015deep}{{10}{2015}{{Hausknecht and Stone}}{{}}}
\bibcite{hochreiter1997long}{{11}{1997}{{Hochreiter and Schmidhuber}}{{}}}
\bibcite{kalchbrenner2016neural}{{12}{2016}{{Kalchbrenner et~al.}}{{Kalchbrenner, Espeholt, Simonyan, Oord, Graves, and Kavukcuoglu}}}
\bibcite{keskar2017large}{{13}{2017}{{Keskar et~al.}}{{Keskar, Mudigere, Nocedal, Smelyanskiy, and Tang}}}
\bibcite{kingma2014adam}{{14}{2014}{{Kingma and Ba}}{{}}}
\bibcite{ladner1980parallel}{{15}{1980}{{Ladner and Fischer}}{{}}}
\bibcite{lecun1998mnist}{{16}{1998}{{LeCun et~al.}}{{LeCun, Cortes, and Burges}}}
\bibcite{orchard2015converting}{{17}{2015}{{Orchard et~al.}}{{Orchard, Jayawant, Cohen, and Thakor}}}
\bibcite{sutskever2014sequence}{{18}{2014}{{Sutskever et~al.}}{{Sutskever, Vinyals, and Le}}}
\bibcite{van2016wavenet}{{19}{2016}{{van~den Oord et~al.}}{{van~den Oord, Dieleman, Zen, Simonyan, Vinyals, Graves, Kalchbrenner, Senior, and Kavukcuoglu}}}
\bibcite{lei2017}{{20}{2017}{{Lei and Zhang }}{{}}}
\bibcite{physiobank}{{21}{2000}{{Goldberger et~al.}}{{ Amaral, Glass, Hausdorff, Ivanov, Mark, Mietus, Moody, Peng, Stanley}}}
