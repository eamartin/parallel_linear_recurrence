\relax 
\providecommand\hyper@newdestlabel[2]{}
\bibstyle{natbib}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{sutskever2014sequence}
\citation{amodei2015deep}
\citation{hausknecht2015deep}
\citation{hochreiter1997long}
\citation{cho2014learning}
\citation{keskar2017large}
\citation{diamos2016persistent}
\citation{balduzzi2016strongly}
\citation{bradbury2017quasi}
\citation{kalchbrenner2016neural}
\citation{gehring2017convolutional}
\citation{van2016wavenet}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{ladner1980parallel}
\citation{blelloch1990prefix}
\citation{bradbury2017quasi}
\citation{lei2017}
\citation{blelloch1990prefix}
\@writefile{toc}{\contentsline {section}{\numberline {2}Parallel linear recurrence}{2}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Theoretical performance}{2}{subsection.2.1}}
\citation{abadi2016tensorflow}
\newlabel{alg:plr}{{2}{3}{Parallel linear recurrence}{section.2}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Parallel linear recurrence on $p$ processors}}{3}{algorithm.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Backpropagation}{3}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Implementation}{3}{subsection.2.3}}
\citation{bradbury2017quasi}
\citation{balduzzi2016strongly}
\@writefile{toc}{\contentsline {section}{\numberline {3}Models}{4}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Gated impulse linear recurrent layer}{4}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Impact on effective "batch size"}{4}{subsubsection.3.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Linear surrogate RNNs}{4}{subsection.3.2}}
\newlabel{sec:ls-rnns}{{3.2}{4}{Linear surrogate RNNs}{subsection.3.2}{}}
\citation{hochreiter1997long}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{5}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Throughput benchmarks}{5}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Kernel performance}{5}{subsubsection.4.1.1}}
\citation{hochreiter1997long}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Parallel kernel speedup on $m$ features (minibatch size $= 1$)}}{6}{table.1}}
\newlabel{table:kernel-throughput}{{1}{6}{Parallel kernel speedup on $m$ features (minibatch size $= 1$)}{table.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Parallel kernel speedup for a variety of LS-RNNs, implemented as two stacked RNN layers with 256 hidden units. We keep the GPU memory usage constant by fixing $bT = 65,536$ for minibatch size $b$ and sequence length $T$}}{6}{table.2}}
\newlabel{table:rnn-throughput}{{2}{6}{Parallel kernel speedup for a variety of LS-RNNs, implemented as two stacked RNN layers with 256 hidden units. We keep the GPU memory usage constant by fixing $bT = 65,536$ for minibatch size $b$ and sequence length $T$}{table.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Accelerating existing RNN architectures}{6}{subsubsection.4.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Synthetic Experiment}{6}{subsection.4.2}}
\citation{hochreiter1997long}
\citation{hochreiter1997long}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The structure of the synthetic example and the GILR-LSTM architecture we used to tackle it. We feed in one-hot unit vectors \(x\) which are chosen uniformly at random (with replacement). The class is determined by the very first vector \(x_0\), which has a fixed direction. The sign of \(x_0\) determines the class. In the diagram, each rounded block indicates a cell of the RNN, whilst the square indicates a linear unit.}}{7}{figure.1}}
\newlabel{fig:synthetic_diagram}{{1}{7}{The structure of the synthetic example and the GILR-LSTM architecture we used to tackle it. We feed in one-hot unit vectors \(x\) which are chosen uniformly at random (with replacement). The class is determined by the very first vector \(x_0\), which has a fixed direction. The sign of \(x_0\) determines the class. In the diagram, each rounded block indicates a cell of the RNN, whilst the square indicates a linear unit}{figure.1}{}}
\newlabel{table:synth-table}{{4.2}{7}{Synthetic Experiment}{figure.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Performance of the GILR-LSTM compared to the CuDNN LSTM on problem 2b from \citet  {hochreiter1997long}. }}{7}{table.3}}
\bibcite{abadi2016tensorflow}{{1}{2016}{{Abadi et~al.}}{{Abadi, Agarwal, Barham, Brevdo, Chen, Citro, Corrado, Davis, Dean, Devin, et~al.}}}
\bibcite{amodei2015deep}{{2}{2015}{{Amodei et~al.}}{{Amodei, Anubhai, Battenberg, Case, Casper, Catanzaro, Chen, Chrzanowski, Coates, Diamos, et~al.}}}
\bibcite{balduzzi2016strongly}{{3}{2016}{{Balduzzi and Ghifary}}{{}}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Learning curves for GILR-LSTM and CuDNN LSTM architectures for various sequence lengths. Each plot shows the moving mean and standard deviation of classification accuracy over five training runs, with the exception of a single run for CuDNN LSTM on 1 million sequence length.}}{8}{figure.2}}
\newlabel{fig:synthetic_training}{{2}{8}{Learning curves for GILR-LSTM and CuDNN LSTM architectures for various sequence lengths. Each plot shows the moving mean and standard deviation of classification accuracy over five training runs, with the exception of a single run for CuDNN LSTM on 1 million sequence length}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{8}{section.5}}
\bibcite{blelloch1990prefix}{{4}{1990}{{Blelloch}}{{}}}
\bibcite{bradbury2017quasi}{{5}{2017}{{Bradbury et~al.}}{{Bradbury, Merity, Xiong, and Socher}}}
\bibcite{cho2014learning}{{6}{2014}{{Cho et~al.}}{{Cho, Van~Merri{\"e}nboer, Gulcehre, Bahdanau, Bougares, Schwenk, and Bengio}}}
\bibcite{diamos2016persistent}{{7}{2016}{{Diamos et~al.}}{{Diamos, Sengupta, Catanzaro, Chrzanowski, Coates, Elsen, Engel, Hannun, and Satheesh}}}
\bibcite{gehring2017convolutional}{{8}{2017}{{Gehring et~al.}}{{Gehring, Auli, Grangier, Yarats, and Dauphin}}}
\bibcite{glorot2010understanding}{{9}{2010}{{Glorot and Bengio}}{{}}}
\bibcite{hausknecht2015deep}{{10}{2015}{{Hausknecht and Stone}}{{}}}
\bibcite{hochreiter1997long}{{11}{1997}{{Hochreiter and Schmidhuber}}{{}}}
\bibcite{kalchbrenner2016neural}{{12}{2016}{{Kalchbrenner et~al.}}{{Kalchbrenner, Espeholt, Simonyan, Oord, Graves, and Kavukcuoglu}}}
\bibcite{keskar2017large}{{13}{2017}{{Keskar et~al.}}{{Keskar, Mudigere, Nocedal, Smelyanskiy, and Tang}}}
\bibcite{kingma2014adam}{{14}{2014}{{Kingma and Ba}}{{}}}
\bibcite{ladner1980parallel}{{15}{1980}{{Ladner and Fischer}}{{}}}
\bibcite{lecun1998mnist}{{16}{1998}{{LeCun et~al.}}{{LeCun, Cortes, and Burges}}}
\bibcite{orchard2015converting}{{17}{2015}{{Orchard et~al.}}{{Orchard, Jayawant, Cohen, and Thakor}}}
\bibcite{sutskever2014sequence}{{18}{2014}{{Sutskever et~al.}}{{Sutskever, Vinyals, and Le}}}
\bibcite{van2016wavenet}{{19}{2016}{{van~den Oord et~al.}}{{van~den Oord, Dieleman, Zen, Simonyan, Vinyals, Graves, Kalchbrenner, Senior, and Kavukcuoglu}}}
\bibcite{lei2017}{{20}{2017}{{Lei and Zhang}}{{}}}
\bibcite{physiobank}{{21}{2000}{{Goldberger et~al.}}{{ Amaral, Glass, Hausdorff, Ivanov, Mark, Mietus, Moody, Peng, Stanley}}}
