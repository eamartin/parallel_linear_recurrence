\relax 
\providecommand\hyper@newdestlabel[2]{}
\bibstyle{abbrvnat}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{sutskever2014sequence}
\citation{amodei2015deep}
\citation{hausknecht2015deep}
\citation{hochreiter1997long}
\citation{cho2014learning}
\citation{keskar2017large}
\citation{diamos2016persistent}
\citation{balduzzi2016strongly}
\citation{bradbury2017quasi}
\citation{kalchbrenner2016neural}
\citation{gehring2017convolutional}
\citation{van2016wavenet}
\@writefile{tdo}{\contentsline {todo}{Thought: we could rephrase this as allowing us to break the necessity of using large batches to train fast}{1}{section*.1}}
\pgfsyspdfmark {pgfid1}{13351168}{31411448}
\pgfsyspdfmark {pgfid4}{35988437}{31403580}
\pgfsyspdfmark {pgfid5}{38101972}{31190265}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{ladner1980parallel}
\citation{blelloch1990prefix}
\citation{blelloch1990prefix}
\@writefile{tdo}{\contentsline {todo}{Seems like an aside - take out?}{2}{section*.2}}
\pgfsyspdfmark {pgfid6}{20573211}{41905357}
\pgfsyspdfmark {pgfid9}{35988437}{41897489}
\pgfsyspdfmark {pgfid10}{38101972}{41684174}
\@writefile{toc}{\contentsline {section}{\numberline {2}Parallel linear recurrence}{2}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Theoretical performance}{2}{subsection.2.1}}
\newlabel{alg:plr}{{2}{3}{Parallel linear recurrence}{section.2}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Parallel linear recurrence on $p$ processors}}{3}{algorithm.1}}
\@writefile{tdo}{\contentsline {todo}{Can include the actual speedup factor, \(\alpha = \frac  {PT}{3(T + \qopname  \relax o{lg}(p))}\).}{3}{section*.3}}
\pgfsyspdfmark {pgfid11}{25837263}{29294825}
\pgfsyspdfmark {pgfid14}{35988437}{29286957}
\pgfsyspdfmark {pgfid15}{38101972}{29073642}
\@writefile{tdo}{\contentsline {todo}{I can't seem to find anyone explicitly saying this before from a quick google.}{3}{section*.4}}
\pgfsyspdfmark {pgfid16}{18794199}{24628663}
\pgfsyspdfmark {pgfid19}{35988437}{24620795}
\pgfsyspdfmark {pgfid20}{38101972}{24407480}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Backpropagation}{3}{subsection.2.2}}
\citation{abadi2016tensorflow}
\citation{bradbury2017quasi}
\citation{balduzzi2016strongly}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Implementation}{4}{subsection.2.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Models}{4}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Gated impulse linear recurrent layer}{4}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Impact on effective "batch size"}{4}{subsubsection.3.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Linear surrogate RNNs}{4}{subsection.3.2}}
\newlabel{sec:ls-rnns}{{3.2}{4}{Linear surrogate RNNs}{subsection.3.2}{}}
\citation{hochreiter1997long}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{5}{section.4}}
\@writefile{tdo}{\contentsline {todo}{I hope so\ldots  }{5}{section*.5}}
\pgfsyspdfmark {pgfid21}{13923358}{13698202}
\pgfsyspdfmark {pgfid24}{35988437}{13690334}
\pgfsyspdfmark {pgfid25}{38101972}{13477019}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Throughput Benchmarks}{5}{subsection.4.1}}
\@writefile{tdo}{\contentsline {todo}{Important: What's the batch size here? Is it just one sequence? Need to say that}{5}{section*.6}}
\pgfsyspdfmark {pgfid26}{17046105}{4957470}
\pgfsyspdfmark {pgfid29}{35988437}{4949602}
\pgfsyspdfmark {pgfid30}{38101972}{4736287}
\citation{lei2017}
\citation{bradbury2017quasi}
\citation{lei2017}
\citation{bradbury2017quasi}
\citation{keskar2017large}
\citation{hochreiter1997long}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Speedup of the PLR kernel compared to the SLR kernel, without any overhead from Tensorflow, for various sequence lengths and input size \(m\).}}{6}{table.1}}
\newlabel{table:kernel-throughput}{{1}{6}{Speedup of the PLR kernel compared to the SLR kernel, without any overhead from Tensorflow, for various sequence lengths and input size \(m\)}{table.1}{}}
\newlabel{table:rnn-throughput}{{4.1}{6}{Throughput Benchmarks}{table.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Speedup obtained when using parallel linear recurrence compared to serial linear recurrence. We analyse this for several recent architectures: the Simple Recurrent Unit from \cite  {lei2017}, the Quasi-RNN from \cite  {bradbury2017quasi}, with a convolutional filter of size 2 and 10, and the GILR-LSTM which we introduce in section \ref  {sec:ls-rnns}. To give a fair model of a typical use-case maximising the memory usage of the GPU, we set batch size \(b\) such that \(Tb = 65536\) for a sequence length \(T\).}}{6}{figure.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Synthetic Example}{6}{subsection.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Throughput comparison between LSTM-256-256 with GILR-LSTM-256-256, with a 32-dimensional input and a 2-dimensional output, for various batch sizes and sequence lengths.The LSTM only has a single row of data because its throughput is independent of sequence length. Entries are missing from the GILR-LSTM table because there was not enough memory on the GPU to handle such large batch sizes and sequences.}}{7}{figure.2}}
\newlabel{fig:tp_perf}{{2}{7}{Throughput comparison between LSTM-256-256 with GILR-LSTM-256-256, with a 32-dimensional input and a 2-dimensional output, for various batch sizes and sequence lengths.The LSTM only has a single row of data because its throughput is independent of sequence length. Entries are missing from the GILR-LSTM table because there was not enough memory on the GPU to handle such large batch sizes and sequences}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Medical Example}{7}{subsection.4.3}}
\@writefile{tdo}{\contentsline {todo}{CITE}{7}{section*.7}}
\pgfsyspdfmark {pgfid31}{29086296}{7110327}
\pgfsyspdfmark {pgfid34}{35988437}{7102459}
\pgfsyspdfmark {pgfid35}{38101972}{6889144}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Training curves for GILR-LSTM and CUDNNLSTM architectures for various sequence lengths. (Clockwise from top-left: 1024, 8192, 1048576)}}{8}{figure.3}}
\newlabel{fig:synthetic_training}{{3}{8}{Training curves for GILR-LSTM and CUDNNLSTM architectures for various sequence lengths. (Clockwise from top-left: 1024, 8192, 1048576)}{figure.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Performance of the GILR-LSTM compared to CUDA-optimised CudnnLSTM implementation on problem 2b from (cite '97). We see that the GILR-LSTM has a clear advantage in training speed as the sequence length increases. We did not observe the Cudnn-LSTM converging for the longest task, even after several days. {\relax \fontsize  {7}{8}\selectfont  *For the longest sequence length, the number of hidden units was decreased to 64 for both architectures so that the net could fit in memory. } }}{8}{table.2}}
\newlabel{my-label}{{2}{8}{Performance of the GILR-LSTM compared to CUDA-optimised CudnnLSTM implementation on problem 2b from (cite '97). We see that the GILR-LSTM has a clear advantage in training speed as the sequence length increases. We did not observe the Cudnn-LSTM converging for the longest task, even after several days. {\scriptsize *For the longest sequence length, the number of hidden units was decreased to 64 for both architectures so that the net could fit in memory. }}{table.2}{}}
\bibcite{abadi2016tensorflow}{{1}{2016}{{Abadi et~al.}}{{Abadi, Agarwal, Barham, Brevdo, Chen, Citro, Corrado, Davis, Dean, Devin, et~al.}}}
\bibcite{amodei2015deep}{{2}{2015}{{Amodei et~al.}}{{Amodei, Anubhai, Battenberg, Case, Casper, Catanzaro, Chen, Chrzanowski, Coates, Diamos, et~al.}}}
\bibcite{balduzzi2016strongly}{{3}{2016}{{Balduzzi and Ghifary}}{{}}}
\bibcite{blelloch1990prefix}{{4}{1990}{{Blelloch}}{{}}}
\bibcite{bradbury2017quasi}{{5}{2017}{{Bradbury et~al.}}{{Bradbury, Merity, Xiong, and Socher}}}
\bibcite{cho2014learning}{{6}{2014}{{Cho et~al.}}{{Cho, Van~Merri{\"e}nboer, Gulcehre, Bahdanau, Bougares, Schwenk, and Bengio}}}
\bibcite{diamos2016persistent}{{7}{2016}{{Diamos et~al.}}{{Diamos, Sengupta, Catanzaro, Chrzanowski, Coates, Elsen, Engel, Hannun, and Satheesh}}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{9}{section.5}}
\bibcite{gehring2017convolutional}{{8}{2017}{{Gehring et~al.}}{{Gehring, Auli, Grangier, Yarats, and Dauphin}}}
\bibcite{glorot2010understanding}{{9}{2010}{{Glorot and Bengio}}{{}}}
\bibcite{hausknecht2015deep}{{10}{2015}{{Hausknecht and Stone}}{{}}}
\bibcite{hochreiter1997long}{{11}{1997}{{Hochreiter and Schmidhuber}}{{}}}
\bibcite{kalchbrenner2016neural}{{12}{2016}{{Kalchbrenner et~al.}}{{Kalchbrenner, Espeholt, Simonyan, Oord, Graves, and Kavukcuoglu}}}
\bibcite{keskar2017large}{{13}{2017}{{Keskar et~al.}}{{Keskar, Mudigere, Nocedal, Smelyanskiy, and Tang}}}
\bibcite{kingma2014adam}{{14}{2014}{{Kingma and Ba}}{{}}}
\bibcite{ladner1980parallel}{{15}{1980}{{Ladner and Fischer}}{{}}}
\bibcite{lecun1998mnist}{{16}{1998}{{LeCun et~al.}}{{LeCun, Cortes, and Burges}}}
\bibcite{orchard2015converting}{{17}{2015}{{Orchard et~al.}}{{Orchard, Jayawant, Cohen, and Thakor}}}
\bibcite{sutskever2014sequence}{{18}{2014}{{Sutskever et~al.}}{{Sutskever, Vinyals, and Le}}}
\bibcite{van2016wavenet}{{19}{2016}{{van~den Oord et~al.}}{{van~den Oord, Dieleman, Zen, Simonyan, Vinyals, Graves, Kalchbrenner, Senior, and Kavukcuoglu}}}
\bibcite{lei2017}{{20}{2017}{{Lei and Zhang }}{{}}}
